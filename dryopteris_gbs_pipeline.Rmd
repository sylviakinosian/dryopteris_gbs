---
title: "*Dryopteris* RADSeq Pipeline"
author: "Sylvia Kinosian"
output: html_document
---

Processing the raw Illumina data was conducted in roughly three steps: 1) creation of a pseudo-reference genome, 2) alignment of reads and variant calling, and 3) admixture analysis.

# {.tabset}

## RADSeq Pipeline
  
This portion of the analysis was done on the [HiperGator](https://www.rc.ufl.edu/services/hipergator/) at the University of Florida, Gainsville.

### 1) Parse barcodes, split .FASTQ by individual, combine paired end reads

This first step uses Perl (v. 5, http://www.perl.org) scripts to parse the barcodes from the raw GBS .FASTQ data and then split that raw file into indiviual .FASTQ files.

#### a. Parse barcodes 
The first scipt, parse\_barcodes768.pl, requires two files: a barcodes file and the raw GBS.FASTQ file. The barcodes.txt file has three columns: the index name, barcode, and sample name for each indiviual. See barcodes.txt for an example. 

Usage:
```{bash eval=FALSE}
perl parse_barcodes768.pl barcodes.txt raw_reads.fastq
```

#### b. Split FASTQ by individual
The next script, splitFastq\_ms.pl, requires two files: a list of individuals and the parsed.FASTQ file (one of the output file from parse\_barcodes768.pl). The individuals.txt file is simply a list of the names for each individual you would like to create a .FASTQ file for (a text file contain one column, with one name per row, no header).

Usage:
```{bash eval=FALSE}
perl splitFastq_ms.pl individuals.txt parsed_raw_reads.fastq
```
Now, you should have a .FASTQ file for each individual listed in the indivduals.txt file. 

#### c. Combine paired-ends reads
Combine paired-end reads for each individual using PEAR: Paired-End reAd mergeR (v. 0.9.8)

```{bash eval=FALSE}
./pear -f forward_read.fastq -r reverse_read.fastq -o outfile_name
```

### 2) Building our reference

### PLEASE NOTE: this is for the diploid species ONLY (*D. goldiana*, *D. intermedia*, *D. expansa* and *D. ludoviciana*). 

Now that we have extracted the raw information for each individual, it's time to build our reference genome. There is no reference genome available for *Dryopteris*, so we are going to build one *de novo* using the two diploid species. Since the tetraploids are allotetraploids (of hybrid origin) we are only using the diploids because that will capture most of the sequence variation and is cleaner than dealing with the possibly divergent sequences on the tetraploid hybrids.

We decided to build the reference genome by clustering similar sequences within species, and then combining those sequences across individuals.

**Before starting**, transfer the .FASTQ files for the diploid individuals to separate folders. This will make the clustering within each species much easier.

### Step 1: Cluster highly similar sequences in individual .FASTQ files

#### a. Convert FASTQ to FASTA
The first part of this step is to convert the .FASTQ files into .FASTA files using the program SEQTK (v. 1.2-r102-dirty). See the script seqtk.sh (below) for an example of how to loop through the files in a directory.

```{bash eval=FALSE}
for i in *.fastq; do
    id=$(echo $i | cut -f1 -d.)
    echo $id
	seqtk seq -a $i > $id.fasta
	done
```

#### b. Create centroids
The second part is to use the program VSEARCH (v. 2.4.2) to cluster sequences **within each individual** with a 98% similarity to create "centroids" or highly similar sequences with which to base the pseudo-reference.

Do this for each individual of each diploid species.

usage of vsearch.sh:

```{bash eval=FALSE}
vsearch.sh 0.98 centroids
```

VSEARCH command "under the hood" of vsearch.sh:
```{bash eval=FALSE}
vsearch --cluster_fast indivdual.fasta --id 0.98 --iddef 2 --threads 8 --centroids centroids/centroids98_individual.fasta
```

### Step 2: combine centroids from preceeding runs and cluster at 92% similarity

#### a. *D. goldiana*
```{bash eval=FALSE}
cat centroids*fasta > goldiana_consensus.fasta

vsearch --cluster_fast goldiana_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92goldianaCons.fasta --msaout 92goldianaMsa.fasta
```

#### b. *D. intermedia*
```{bash eval=FALSE}
cat centroids*.fasta intermedia_consensus.fasta

vsearch --cluster_fast intermedia_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92intermediaCons.fasta --msaout 92intermediaMsa.fasta
```

#### c. *D. expansa*
```{bash eval=FALSE}
cat centroids*.fasta expansa_consensus.fasta

vsearch --cluster_fast expansa_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92expansaCons.fasta --msaout 92expansaMsa.fasta
```

#### b. *D. ludoviciana*
```{bash eval=FALSE}
cat centroids*.fasta ludoviciana_consensus.fasta

vsearch --cluster_fast ludoviciana_consensus.fasta --threads 10 --iddef 2 --id 0.92 --consout 92ludovicianaCons.fasta --msaout 92ludovicianaMsa.fasta
```

### Step 3: using the output from clustering at 92%, cluster again at 84% similarity

#### a. *D. goldiana*
```{bash eval=FALSE}
vsearch --cluster_fast 92goldianaCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84goldianaCons.fasta --msaout 84goldianaMsa.fasta
```

#### b. *D. intermedia*
```{bash eval=FALSE}
vsearch --cluster_fast 92intermediaCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84intermediaCons.fasta --msaout 84intermediaMsa.fasta
```

#### c. *D. expansa*
```{bash eval=FALSE}
vsearch --cluster_fast 92expansaCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84expansaCons.fasta --msaout 84expansaMsa.fasta
```

####b. *D. ludoviciana*
```{bash eval=FALSE}
vsearch --cluster_fast 92ludovicianaCons.fasta --threads 10 --iddef 2 --id 0.84 --consout 84ludovicianaCons.fasta --msaout 84ludovicianaMsa.fasta
```

### Step 4: Remove collapsed clusters (paralogs) from files clustered at 84% similarity

The remove\_collapsed\_clusters.py script removes all entries that have (the 2nd) seqs > 1.

#### a. *D. goldiana*
```{bash eval=FALSE}
./remove_collapsed_clusters.py 84goldianaCons.fasta RCCgoldiana.fasta
sta

440468 uncollapsed clusters found
```

#### b. *D. intermedia*
```{bash eval=FALSE}
./remove_collapsed_clusters.py 84intermediaCons.fasta RCCintermedia.fasta

837895 uncollapsed clusters found
```

#### c. *D. expansa*
```{bash eval=FALSE}
./remove_collapsed_clusters.py 84expansaCons.fasta RCCexpansa.fasta

64631 uncollapsed clusters found
```

#### b. *D. ludoviciana*
```{bash eval=FALSE}
./remove_collapsed_clusters.py 84ludovicianaCons.fasta RCCludoviciana.fasta

132480 uncollapsed clusters found
```

The resulting RCC\*.fasta files will be used in Step 5.

### Step 5: combine the diploid species, re-run VSEARCH clustering and filter

#### a. BEFORE COMBINING
Make sure your *D. goldiana*, *D. intermedia*, *D. expansa*, and *D. ludoviciana* individuals are marked separatley within the RCC\*.fasta files. This will make checking to see if each final contig is represented by each species much easier.

so far have not used this....
```{bash eval=FALSE}
sed 's/^>centroid=centroid=/>centroid=centroid=g/g' RCCgoldiana.fasta > gRCCgoldiana.fasta

sed 's/^>centroid=centroid=/>centroid=centroid=i/g' RCCintermedia.fasta > iRCCintermedia.fasta

sed 's/^>centroid=centroid=/>centroid=centroid=e/g' RCCexpansa.fasta > eRCCexpansa.fasta

sed 's/^>centroid=centroid=/>centroid=centroid=l/g' RCCludoviciana.fasta > lRCCludoviciana.fasta
```

#### a. Combine the diploids

*intermedia* and *expansa*
```{bash eval=FALSE}
cat iRCCintermedia.fasta eRCCexpansa.fasta > ex_int_cons.fasta
```

*goldiana* and *ludoviciana*
```{bash eval=FALSE}
cat gRCCgoldiana.fasta lRCCludoviciana.fasta > g_l_cons.fasta
```

#### b. Re-run vsearch with 84% similarity

```{bash eval=FALSE}
# cluster at 84% similarity
vsearch --cluster_fast diploid_consensus.fasta --threads 10 --iddef 2 --id 0.84 --consout 84diploidCons.fasta --msaout 84diploidMsa.fasta

# make sure each contig is present in each diploid
./presence_filter.pl ex_int_cons.fasta

# remove blank contigs
grep -i -B 1 -E '^[[:upper:]]+$' contigs_84eiMsa.fasta > ei_ref.fasta

# using vim, remove -- lines
# :g/^--$/d
```

We chose to use the 84% similarity. The proided us with a large number of contigs present in both species. We are going to apply some much stricter filtering parameters later on in the variant calling step, so it is better to start off with a few more contigs / material in general.

Copy and rename 84diploidCons --> diploid\_consensus\_final.fasta

Hooray! Now you have a fresh *de novo* reference with which to align your parsed .FASTQ files!

## Alignment of Reads

This portion of the analysis was done on the [Center for High Performance Computing](https://www.chpc.utah.edu/) at the University of Utah.

### Step 1: Prepare the reference sequence

#### a. Index the reference (consensus) sequence. 

Here, we used the Burrow-Wheeler Aligner (BWA v. 0.7.10) to index our reference genome. This give the squence position points for the alignment later on.

```{bash eval=FALSE}
bwa index diploid_consensus_final.fasta 
```

#### b. Picard tools to create a dictionary

We used Java (OpenJDK) v. 1.8.0 and PicardTools v. 2.9.0

```{bash eval=FALSE}
java -jar picard.jar CreateSequenceDictionary REFERENCE=diploid_consensus_final.fasta OUTPUT=diploid_consensus_final.dict
```
#### c. Creating the fasta index file

We used SAMTOOLS v. 1.5

```{bash eval=FALSE}
samtools faidx diploid_consensus_final.fasta
```

Now you should have the following files:

consensus.dict
consensus.fasta
consensus.amb
consensus.ann
consensus.bwt
consensus.fai
consensus.pac
consensus.sa

### Step 2: Align parsed reads (from ALL individual .FASTQ files) to the *de novo* reference

#### a. Align individuals with BWA ALN

See script `bwa_aln.sh`

```{bash eval=FALSE}
#!/bin/bash

REF='/path/to/consensus_final.fasta'

for i in *.fastq;
do
ids=$(echo $i | cut -f1 -d.)
echo $ids

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa aln -n 4 -l 20 -k 2 -t 8 -q 10 -f $ids.sai $REF $i

/uufs/chpc.utah.edu/common/home/u6009816/apps/bwa-0.7.15/bwa samse -n 1 -r "@RG\tID:$ids\tLB:$ids\tSM:$ids\tPL:ILLUMINA" -f $ids.sam $REF $ids.sai $i

done
```

The output is a .SAM file for each individual

#### b. Convert files from .SAM to .BAM, sort, and index the individuals using SAMTOOLS

```{bash eval=FALSE}
samtools view -o *.bam *.sam

samtools sort -o *.sorted.bam *.bam
 
samtools index -b *.sorted.bam
```

### Step 3: Call Variants

To call variants, we used the GATK HaplotypeCaller (v. 4) because of its ability to specify ploidy. We called variants separately for the diploids and tetraploids.

samtools mpileup and bcftools call??
#### Diploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R diploid_consensus_final -I diploid_bams.list --genotyping_mode DISCOVERY -ploidy 2 -o 2ae_rawVar.vcf -out_mode EMIT_VARIANTS_ONLY
```

<!--
#### Tetraploids
```{bash eval=FALSE}
java -Xmx48g -jar GenomeAnaysisTK.jar -T HaplotypeCaller -R diploid_consensus_final -I tetraploid_bams.list --genotyping_mode DISCOVERY -ploidy 4 -o 4ae_rawVar.g.vcf -out_mode EMIT_VARIANTS_ONLY --variant_index_type LINEAR --variant_index_parameter 128000
```

```{bash eval=FALSE}
grep -v ^# ae.vcf | cut -f 8 | perl -p -i -e 's/DP=(\d+);\S+/\1/' > depth.txt
```
-->

### Step 4: filter VCFs

```{bash eval=FALSE}
vcftools --remove-filtered-all --remove-indels --maf 0.1 --max-maf 0.99 --min-meanDP 4.0 --max-missing 0.3 --minQ 20 --recode-INFO-all --recode --vcf rawVar.vcf
```
Kept 2345 out of a possible 14593 sites

The script vcfFilter.py filters based on read depth (minCoverage), alternative alleles(minAltRds), fixed loci (notFixed), and mapping quality (mapQual). These variables can be altered within the file to achieve the desired filtering affect (see **stringency variable** in script below). 
 
<!--
BOTH VCF files need to be filtered this way (2\_rawVar.vcf and 4\_rawVar.vcf).

### Step 5: Find the intersection of variants in diploids and tetraploids

Since we now have two VCF files, we need to combine them again somehow. To do this, we find the **intersection** of the variants in both files, subset, and then re-combine.

#### a. Intersection of variants

```{bash eval=FALSE}
perl vcf_checker.pl filtered_2.vcf filtered_4.vcf
```

Output is a list of the contigs present in both VCF files called matches.txt

#### b. Subset VCF files with list of intersection matches

Do this for both files.

```{bash eval=FALSE}
perl subsetVcf.pl matches.vcf filtered_2.vcf

perl subsetVcf.pl matches.vcf filtered_4.vcf
```

Output is two files: sub\_filtered\_2.vcf and sub\_filtered\_4.vcf

#### c. Combine files

What we are doing here is appending the data from one file onto the end of each matching contig in the other file.

```{bash eval=FALSE}
perl combine.pl sub\_filtered\_2.vcf sub\_filtered\_4.vcf
```

This outputs a file called diploidsAll.vcf

**Add CHROM lines from each to diploidsAll.vcf** -->

## Population genetics

### **Entropy**

Before getting started with ENTROPY, we need to convert our VCF file to a GL (Genotype Likelihood) file.

We used the perl script vcf2gl.pl to convert our filtered vcf to the simpler .gl format for downstream analysis.

For diploids:

```{bash eval=FALSE}
perl vcf2gl.pl aeAll.vcf
```
This outputs a file called out.recode.gl

Next we are going to convert the GL file to a matrix that we can use in R with DAPC.

```{bash eval=FALSE}
perl gl2genest.pl out.recode.gl
```
This outputs a file called pntest\_out.recode.gl

### Discriminant Analysis of Principle Components 

##### a. seed entropy with values from DAPC

Using the R package ADEGENET (v. 2.1.1), run a Discriminate Analysis of Principle Componets (DAPC function) to seed values in ENTROPY so we don't get label swapping. We followed the [DAPC vignette](adegenet.r-forge.r-project.org/files/tutorial-dapc.pdf).

```{r eval=FALSE}
library(adegenet)

# read in genotype matrix
d <- read.table("pntest_out.recode.vcf", header = F)

# transform data
dt <- t(d)

# convert to genind object
dg <- df2genind(dt, sep = " ", ploidy = 2)

grp <- find.clusters(dg, max.n.clust = 6)
# number of PCs retained: 
# number of clusters: 

head(grp$grp, 8)

# get likelihood assignments

dapc1 <- dapc(dg, grp$grp)
# PCs: 
# discriminant functions:

write.table(dapc1$posterior, "k_est.txt")
```

##### b. run ENTROPY

```{bash eval=FALSE}
./entropy -b 10000 -l 50000 -t 4 -k 2 -i ae_in.gl -o out.hdf5 -m 1 -w 0 -q pop_ests.txt -s 20
```
-b discard the first n MCMC samples as a burn-in
-t Thin MCMC samples by recording everyth nth value
-k number of population clusters
-i GL format infile
-o HDF5 format outfile
-m infile is in genotype likelihood format 
-w output includes population allele frequencies
-q file with expected starting values for admixture proportions
-s scalar for Dirichlet init of q, inversly proportional to variance

##### c. ESTPOST - pulling out meaningful things from entropy

```{bash eval=FALSE}
./estpost_entropy -o k2.txt -p q -s 0 -w 1 entropy_ae_k2_2.hdf5
```
-o outfile
-p name of parameter to summarize
-s which summary to perform: 0 = posterior estimates and credible intervals
                             1 = histogram of posterior samples
                             2 = convert to plain text
                             3 = calculate DIC
                             4 = MCMC diagnostic
- w write parameter identification to file, boolean

### **Visualizing Admixture**

```{r eval=FALSE}
##############################
# custom functions
# read in to R before starting
##############################

# averages up to 3 chains for a given k
avg_k <- function(kval, n_inds = 16, chain1, chain2 = chain1, chain3 = chain1){
	df <- as.data.frame(matrix(nrow = kval*n_inds,  ncol = 4))
	colnames(df)[1:4] <- c("chain1", "chain2", "chain3", "avg")
	df[,1] <- chain1[,1]	
	df[,2] <- chain2[,1]
	df[,3] <- chain3[,1]
	df[,4] <- rowMeans(df[sapply(df, is.numeric)])
	return(df)
}

# makes a data frame with your averaged chains
make_df <- function(kfile, kval, ninds = 16, names){
	x <- 1
	df <- as.data.frame(matrix(nrow = ninds, ncol = kval+x))
	for (i in 1:kval){
		df[,i] <- kfile[x:(ninds*i),4]
		x <- x+ninds
	}
	df[,ncol(df)] <- names
	df <- df[order(df[,ncol(df)]),]
	return(df)
}

# function to plot each chain for a given k
plot_q_per_chain <- function(kqlist, xlabel, ...){
	cols <- c('#D62227','#F38020','#F5C527','#8FBA3E','#40B8D8','#71449A')	
	par(mfrow= c(length(kqlist),1), mar=c(4,2,1,1) + 0.1, oma= c(5,0,0,0), mgp= c(0,1,0))
	chain <- seq(1, length(kqlist), 1) 
	for(i in 1:length(kqlist)){
		barplot(t(kqlist[[i]]), beside= F, col= cols, las= 2, axisnames= T, cex.name= 1, cex.axis= 1.2, border= 1, space= c(0.05,0.05), yaxt= 'n', ylab= paste("k =", chain[i+1], sep= ' '), cex.lab= 2, names.arg= xlabel)
		axis(2, at= c(0, 0.5, 1), cex.axis= 1, las= 2, pos= -0.2)
 	}
}

################################
# read in all files from estpost
################################

k2_1 <- read.csv("k2_1.txt", sep = ',', header = F)
k2_2 <- read.csv("k2_2.txt", sep = ',', header = F)
k2_3 <- read.csv("k2_3.txt", sep = ',', header = F)

k4 <- read.csv("k_4.txt", sep = ',', header = T)
k4 <- k4[,-1]
k5 <- read.csv("k_5.txt", sep = ',', header = T)
k5 <- k5[,-1]
k6 <- read.csv("k_6.txt", sep = ',', header = T)
k6 <- k6[,-1]

# list of names and IDs for each individual
names <- read.csv("names.txt", sep ',', header = F)

# create data frames for each k
a4 <- avg_k(kval = 4, chain1 = k4)
df4 <- make_df(kfile = a4, kval = 4, names = names)
a5 <- avg_k(kval = 5, chain1 = k5)
df5 <- make_df(kfile = a5, kval = 5, names = names)
a6 <- avg_k(kval = 6, chain1 = k6)
df6 <- make_df(kfile = a6, kval = 6, names = names)

# create list for plot_q_per_chain
k4list <- list(df4[,1:4])
k5list <- list(df5[,1:5])
k6list <- list(df6[,1:6])

# plot each k
plot_q_per_chain(k4list, df4[,5])
```

## Program Versions

Below is a list of all program versions used in this analysis. Please note that newer versions of these software packages *may* work for this pipeline, but be aware that usage often changes with new verions. Read all documentation for each program carefully before using.

[Perl 5](https://www.perl.org/)

[Python 2.7.13](https://www.python.org/downloads/release/python-2713/)

[SAMtools v. 1.5](https://sourceforge.net/projects/samtools/files/samtools/1.5/)

[SEQTK 1.2-r102-dirty](https://github.com/lh3/seqtk)

[VSEARCH 2.4.2](https://github.com/torognes/vsearch)

[BWA 0.7.15](https://sourceforge.net/projects/bio-bwa/files/)

[PicardTools 2.9.0](https://github.com/broadinstitute/picard/releases)

[GATK v.3.8.0](https://software.broadinstitute.org/gatk/download/archive) - [HaplotypeCaller](https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php)

[ENTROPY & ESTPOST](https://github.com/sylviakinosian/Pteridium_GBS_Pipeline/tree/master/entropy)

#
